{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 27s 2us/step\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 48000 samples, validate on 12000 samples\n",
      "Epoch 1/25\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.3397 - accuracy: 0.8973 - val_loss: 0.0970 - val_accuracy: 0.9725\n",
      "Epoch 2/25\n",
      "48000/48000 [==============================] - 88s 2ms/step - loss: 0.0946 - accuracy: 0.9708 - val_loss: 0.0631 - val_accuracy: 0.9818\n",
      "Epoch 3/25\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0662 - accuracy: 0.9798 - val_loss: 0.0564 - val_accuracy: 0.9837\n",
      "Epoch 4/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0526 - accuracy: 0.9842 - val_loss: 0.0421 - val_accuracy: 0.9876\n",
      "Epoch 5/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0431 - accuracy: 0.9865 - val_loss: 0.0387 - val_accuracy: 0.9887\n",
      "Epoch 6/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0347 - accuracy: 0.9892 - val_loss: 0.0359 - val_accuracy: 0.9899\n",
      "Epoch 7/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0313 - accuracy: 0.9895 - val_loss: 0.0361 - val_accuracy: 0.9903\n",
      "Epoch 8/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0269 - accuracy: 0.9912 - val_loss: 0.0320 - val_accuracy: 0.9910\n",
      "Epoch 9/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0223 - accuracy: 0.9927 - val_loss: 0.0321 - val_accuracy: 0.9918\n",
      "Epoch 10/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0196 - accuracy: 0.9937 - val_loss: 0.0330 - val_accuracy: 0.9910\n",
      "Epoch 11/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0170 - accuracy: 0.9945 - val_loss: 0.0363 - val_accuracy: 0.9913\n",
      "Epoch 12/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0175 - accuracy: 0.9944 - val_loss: 0.0334 - val_accuracy: 0.9908\n",
      "Epoch 13/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0144 - accuracy: 0.9954 - val_loss: 0.0311 - val_accuracy: 0.9921\n",
      "Epoch 14/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0130 - accuracy: 0.9956 - val_loss: 0.0325 - val_accuracy: 0.9917\n",
      "Epoch 15/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0114 - accuracy: 0.9963 - val_loss: 0.0333 - val_accuracy: 0.9922\n",
      "Epoch 16/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0115 - accuracy: 0.9960 - val_loss: 0.0314 - val_accuracy: 0.9918\n",
      "Epoch 17/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0092 - accuracy: 0.9969 - val_loss: 0.0341 - val_accuracy: 0.9926\n",
      "Epoch 18/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0085 - accuracy: 0.9974 - val_loss: 0.0332 - val_accuracy: 0.9924\n",
      "Epoch 19/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0066 - accuracy: 0.9979 - val_loss: 0.0366 - val_accuracy: 0.9927\n",
      "Epoch 20/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0075 - accuracy: 0.9973 - val_loss: 0.0382 - val_accuracy: 0.9918\n",
      "Epoch 21/25\n",
      "48000/48000 [==============================] - 87s 2ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0361 - val_accuracy: 0.9918\n",
      "Epoch 22/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0073 - accuracy: 0.9975 - val_loss: 0.0319 - val_accuracy: 0.9930\n",
      "Epoch 23/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0053 - accuracy: 0.9982 - val_loss: 0.0361 - val_accuracy: 0.9927\n",
      "Epoch 24/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0070 - accuracy: 0.9976 - val_loss: 0.0365 - val_accuracy: 0.9923\n",
      "Epoch 25/25\n",
      "48000/48000 [==============================] - 86s 2ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0401 - val_accuracy: 0.9919\n",
      "Test loss for Keras PReLU CNN: 0.0357950520894242 / Test accuracy: 0.9914000034332275\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.initializers import Constant\n",
    "from keras import backend as K\n",
    "from keras.layers import PReLU\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Model configuration\n",
    "img_width, img_height = 28, 28\n",
    "batch_size = 250\n",
    "no_epochs = 25\n",
    "no_classes = 10\n",
    "validation_split = 0.2\n",
    "verbosity = 1\n",
    "leaky_relu_alpha = 0.1\n",
    "\n",
    "# Load MNIST dataset\n",
    "(input_train, target_train), (input_test, target_test) = mnist.load_data()\n",
    "\n",
    "# Reshape data based on channels first / channels last strategy.\n",
    "# This is dependent on whether you use TF, Theano or CNTK as backend.\n",
    "# Source: https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "if K.image_data_format() == 'channels_first':\n",
    "    input_train = input_train.reshape(input_train.shape[0], 1, img_width, img_height)\n",
    "    input_test = input_test.reshape(input_test.shape[0], 1, img_width, img_height)\n",
    "    input_shape = (1, img_width, img_height)\n",
    "else:\n",
    "    input_train = input_train.reshape(input_train.shape[0], img_width, img_height, 1)\n",
    "    input_test = input_test.reshape(input_test.shape[0], img_width, img_height, 1)\n",
    "    input_shape = (img_width, img_height, 1)\n",
    "\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "\n",
    "# Normalize data [0, 1].\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "\n",
    "# Convert target vectors to categorical targets\n",
    "target_train = keras.utils.to_categorical(target_train, no_classes)\n",
    "target_test = keras.utils.to_categorical(target_test, no_classes)\n",
    "\n",
    "\n",
    "# Create the model\n",
    "classifier = Sequential()\n",
    "classifier.add(Conv2D(32, kernel_size=(3, 3), input_shape=input_shape))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256))\n",
    "model.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "model.add(Dense(no_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Fit data to model\n",
    "history = model.fit(input_train, target_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=no_epochs,\n",
    "          verbose=verbosity,\n",
    "          validation_split=validation_split)\n",
    "\n",
    "\n",
    "# Generate generalization metrics\n",
    "score = model.evaluate(input_test, target_test, verbose=0)\n",
    "print(f'Test loss for Keras PReLU CNN: {score[0]} / Test accuracy: {score[1]}')\n",
    "\n",
    "# Visualize model history\n",
    "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
    "plt.title('PReLU training / validation accuracies')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.title('PReLU training / validation loss values')\n",
    "plt.ylabel('Loss value')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "Found 347 images belonging to 6 classes.\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/6\n",
      "200/200 [==============================] - 1562s 8s/step - loss: 1.2768 - accuracy: 0.4803\n",
      "Epoch 2/6\n",
      "200/200 [==============================] - 1710s 9s/step - loss: 0.4428 - accuracy: 0.8356\n",
      "Epoch 3/6\n",
      "200/200 [==============================] - 1740s 9s/step - loss: 0.2048 - accuracy: 0.9310\n",
      "Epoch 4/6\n",
      "200/200 [==============================] - 1647s 8s/step - loss: 0.1351 - accuracy: 0.9541\n",
      "Epoch 5/6\n",
      "200/200 [==============================] - 1449s 7s/step - loss: 0.0927 - accuracy: 0.9729\n",
      "Epoch 6/6\n",
      "200/200 [==============================] - 1449s 7s/step - loss: 0.1487 - accuracy: 0.9552\n",
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.models import model_from_json\n",
    "from keras.initializers import Constant\n",
    "from keras import backend as K\n",
    "from keras.layers import PReLU\n",
    "\n",
    "\n",
    "\n",
    "# Initialising the CNN\n",
    "classifier = Sequential()\n",
    "\n",
    "\n",
    "# Step 1.1 - Double Convolution\n",
    "classifier.add(Conv2D(128, kernel_size=(3, 3), input_shape=(64,64,3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "classifier.add(Conv2D(128, kernel_size=(3, 3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "# Step 1.2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "# Step 2.1 - Double Convolution\n",
    "classifier.add(Conv2D(96, kernel_size=(3, 3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "classifier.add(Conv2D(96, kernel_size=(3, 3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "# Step 2.2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "# Step 3.1 - Double Convolution\n",
    "classifier.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "classifier.add(Conv2D(64, kernel_size=(3, 3)))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "# Step 3.2 - Pooling\n",
    "classifier.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "classifier.add(Dropout(0.25))\n",
    "\n",
    "classifier.add(Flatten())\n",
    "classifier.add(Dense(256))\n",
    "classifier.add(PReLU(alpha_initializer=Constant(value=0.25)))\n",
    "\n",
    "classifier.add(Dense(256))\n",
    "classifier.add(LeakyReLU(alpha=0.05))\n",
    "# classifier.add(Dense(units=6, activation='softmax'))\n",
    "\n",
    "classifier.add(Dense(units = 6, activation = 'softmax')) #softmax layer must have the same number of nodes as the output later\n",
    "\n",
    "# Compiling the CNN\n",
    "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Part 2 - Fitting the CNN to the images\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "training_set = train_datagen.flow_from_directory('E:\\\\2 DL\\\\Dataset\\\\ExtendedFamily',\n",
    "                                                 target_size = (64, 64),\n",
    "                                                 batch_size = 32,\n",
    "                                                 class_mode = 'categorical',\n",
    "                                                 subset='training')\n",
    "\n",
    "# test_set = test_datagen.flow_from_directory('E:\\\\2 DL\\\\Dataset\\\\ExtendedFamily',\n",
    "#                                             target_size = (64, 64),\n",
    "#                                             batch_size = 32,\n",
    "#                                             class_mode = 'categorical',\n",
    "#                                             subset='validation')\n",
    "\n",
    "model = classifier.fit_generator(training_set,\n",
    "                         steps_per_epoch = 200,\n",
    "                         epochs = 6)\n",
    "\n",
    "\n",
    "# classifier.save(\"model14.h5\")    #: Step Size 200, 2C, 2D, 0.2V. Acuuracy=0.9903 after 6th Epoch\n",
    "classifier.save(\"model15.h5\")      #: Step Size 200, 6C, 3D, 0.2V. Acuuracy=0.9552 after 6th Epoch\n",
    "print(\"Saved model to disk\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wali\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "# model = load_model('model1.h5')\n",
    "# model = load_model('model2.h5')\n",
    "# model = load_model('model3.h5')\n",
    "# model = load_model('model4.h5') #: Step size 800\n",
    "# model = load_model('model5.h5')\n",
    "# model = load_model('model6.h5') #: Step size 800\n",
    "# model = load_model('model9.h5') #: Step size 300\n",
    "model = load_model('model10.h5')\n",
    "\n",
    "#Input(=input_shape; step 1.1) image size and test(=test_image) image size should be same\n",
    "test_image = image.load_img('C:\\\\Users\\\\TalatWasi\\\\Desktop\\\\Test\\\\Tipu Mamu\\\\Ti7.jpg', target_size = (64, 64))\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n",
    "result = model.predict(test_image)\n",
    "\n",
    "if result[0][0] == 1:\n",
    "    prediction = 'Abbu'\n",
    "    print(prediction) \n",
    "elif result[0][1]==1:\n",
    "    prediction = 'Ali'\n",
    "    print(prediction)\n",
    "elif result[0][2]==1:\n",
    "    prediction = 'Monis'\n",
    "    print(prediction)\n",
    "elif result[0][3]==1:\n",
    "    prediction = 'Nahid Mamu'\n",
    "    print(prediction)\n",
    "elif result[0][4]==1:\n",
    "    prediction = 'Tipu Mamu'\n",
    "    print(prediction)\n",
    "elif result[0][5]==1:\n",
    "    prediction = 'Wali'\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\ProgramData\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Part 3 - Making new predictions\n",
    "# from keras.models import load_model\n",
    "# import numpy as np\n",
    "# from keras.preprocessing import image\n",
    "\n",
    "# # model = load_model('model1.h5')\n",
    "# # model = load_model('model2.h5')\n",
    "# # model = load_model('model3.h5')\n",
    "# # model = load_model('model4.h5') #: Step size 800\n",
    "# # model = load_model('model5.h5')\n",
    "# model = load_model('model6.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 62, 62, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 60, 60, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 30, 30, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 28, 28, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 26, 26, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 10816)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               1384576   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 1,450,918\n",
      "Trainable params: 1,450,918\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 1. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(training_set.class_indices, \"\\n\")\n",
    "# print(test_set.class_indices, \"\\n\")\n",
    "\"\"\"{'Abbu': 0, 'Ali': 1, 'Monis': 2, 'NahidMamu': 3, 'Raddu': 4, 'TipuMamu': 5, 'Umar': 6, 'Wali': 7}\"\"\"\n",
    "print(result)\n",
    "#\"\"\"[[0. 0. 0. 0. 0. 0. 0. 1.]]\"\"\"\n",
    "# print(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Directory Path\n",
    "Dir=\"C:\\\\Users\\\\TalatWasi\\\\Desktop\\\\Test\"\n",
    "\n",
    "# Code to get list of sub directories in the parent directory\n",
    "sub_dir_name_list = os.listdir(Dir)\n",
    "#sub_dir_name_list\n",
    "\n",
    "# Create paths to subdirectories\n",
    "sub_dir_path_list = [os.path.join(Dir, sub_dir_name) for sub_dir_name in sub_dir_name_list]\n",
    "#sub_dir_path_list\n",
    "\n",
    "# Code to alpbetical create prefix for file names for files in sub folders\n",
    "count=1\n",
    "while True:\n",
    "    file_name_prefix_list = [sub_dir_name[0:count] for sub_dir_name in sub_dir_name_list]\n",
    "    if len(file_name_prefix_list) == len(set(file_name_prefix_list)):\n",
    "        break\n",
    "    else:\n",
    "        count=count + 1\n",
    "#file_name_prefix_list\n",
    "\n",
    "# Code to rename files in subfolder\n",
    "\n",
    "# for sub_dir_path, file_name_prefix in dict(zip(sub_dir_path_list, file_name_prefix_list)).items():\n",
    "#     seq=0\n",
    "#     for current_file_name in os.listdir(sub_dir_path): \n",
    "#         os.rename(sub_dir_path + \"\\\\\" + current_file_name, sub_dir_path + \"\\\\\" + file_name_prefix + str(seq) + \".jpg\")\n",
    "#         seq = seq + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Abbu\n",
      "Ali\n",
      "Ali\n",
      "Ali\n",
      "Ali\n",
      "Ali\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Monis\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Nahid Mamu\n",
      "Tipu Mamu\n",
      "Tipu Mamu\n",
      "Tipu Mamu\n",
      "Tipu Mamu\n",
      "Tipu Mamu\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "Wali\n",
      "{'Abbu': 0.9024390243902439, 'Ali': 0.3333333333333333, 'Monis': 0.45614035087719296, 'Nahid Mamu': 0.5882352941176471, 'Tipu Mamu': 0.4166666666666667, 'Wali': 0.875}\n"
     ]
    }
   ],
   "source": [
    "# Part 3 - Making new predictions\n",
    "import os\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "\n",
    "Test_Accuracy={}\n",
    "Abbu=Ali=Monis=NahidMamu=TipuMamu=Wali=0\n",
    "model = load_model('model15.h5') #: Step size 800\n",
    "\n",
    "for k in range(len(file_name_prefix_list)):\n",
    "    folder=sub_dir_path_list[k]\n",
    "    class_name= sub_dir_name_list[k]\n",
    "    file_num= len(os.listdir(folder))\n",
    "    prefix= file_name_prefix_list[k]\n",
    "    \n",
    "    for i in range(file_num):\n",
    "        \n",
    "        file_name=prefix+str(i)+\".jpg\"\n",
    "        file_path = os.path.join(os.path.join(folder, file_name))\n",
    "        #Input(=input_shape; step 1.1) image size and test(=test_image) image size should be same\n",
    "        test_image = image.load_img(file_path, target_size = (64, 64))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = model.predict(test_image)\n",
    "        \n",
    "        if result[0][0] == 1 and prefix=='Abbu'[0:len(prefix)]:\n",
    "            prediction = 'Abbu' #Model 14: 36/41\n",
    "            Abbu= Abbu +1\n",
    "            Accuracy[class_name]=Abbu/file_num\n",
    "#             count=count+1\n",
    "#             print(prediction) \n",
    "        elif result[0][1]==1 and prefix=='Ali'[0:len(prefix)]:\n",
    "            prediction = 'Ali' #Model 14: 9/15\n",
    "            Ali= Ali +1\n",
    "            Accuracy[class_name]=Ali/file_num\n",
    "#             count=count+1\n",
    "#             print(prediction)\n",
    "        elif result[0][2]==1 and prefix=='Monis'[0:len(prefix)]: \n",
    "            prediction = 'Monis' #Model 14: 38/66\n",
    "            Monis= Monis +1\n",
    "            Accuracy[class_name]=Monis/file_num\n",
    "#             count=count+1\n",
    "#             print(prediction)\n",
    "        elif result[0][3]==1 and prefix=='Nahid Mamu'[0:len(prefix)]:\n",
    "            prediction = 'Nahid Mamu' #Model 14: 9/17\n",
    "            NahidMamu= NahidMamu +1\n",
    "            Accuracy[class_name]=NahidMamu/file_num\n",
    "#             print(prediction)                    \n",
    "#             count=count+1\n",
    "        elif result[0][4]==1 and prefix=='Tipu Mamu'[0:len(prefix)]:\n",
    "            prediction = 'Tipu Mamu' #Model 14 9/12\n",
    "            TipuMamu= TipuMamu +1\n",
    "            Accuracy[class_name]=TipuMamu/file_num\n",
    "#             print(prediction)\n",
    "#             count=count+1\n",
    "        elif result[0][5]==1 and prefix=='Wali'[0:len(prefix)]:\n",
    "            prediction = 'Wali' #Model 14: 9/16\n",
    "            Wali=Wali+1\n",
    "            Accuracy[class_name]=Wali/file_num\n",
    "#             print(prediction)\n",
    "#             count=count+1\n",
    "            \n",
    "\n",
    "\n",
    "print(Accuracy)\n",
    "\n",
    "#Model 15: {'Abbu': 0.9024390243902439, 'Ali': 0.3333333333333333, 'Monis': 0.45614035087719296, \n",
    "#            'Nahid Mamu': 0.5882352941176471, 'Tipu Mamu': 0.4166666666666667, 'Wali': 0.875}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\TalatWasi\\\\Desktop\\\\Test\\\\Nahid Mamu\\\\Mo1.jpg'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "folder='C:\\\\Users\\\\TalatWasi\\\\Desktop\\\\Test\\\\Nahid Mamu'\n",
    "file_name=\"Mo\"+str(1)+\".jpg\"\n",
    "file_path = os.path.join(os.path.join(folder, file_name))\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy={}\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abbu': 0.5}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy['Abbu']=8/16\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abbu': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy['Abbu']=14/7\n",
    "Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Abbu': 0.5, 'Ali': 0.9}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Accuracy['Ali']=9/10\n",
    "Accuracy"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
